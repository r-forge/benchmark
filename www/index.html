
<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en   ">

  <head>
  	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Benchmark Experiments</title>
	<link rel="stylesheet" type="text/css" href="mjaepkg.css" />
  </head>

<body>
  <h1>Benchmark Experiments</h1>
  <div id="h1sub">R package <tt>benchmark</tt></div>

  <p class="header"></p>

  <p>
    In statistical learning benchmarking is the methodology of
    comparing learners or algorithms with respect to a certain
    performance measure. The benchmarking process abstractly consists
    of three levels: Setup, Execution and Analysis. (1) The Setup
    defines the design of a benchmark experiment; data set, candidate
    algorithms, performance measures and a suitable resampling
    strategy are declared. (2) In the Execution level the defined
    setup is executed. Here, computational aspects play a major role;
    an important example is the parallel computation of the experiment
    on different computers. (3) In the Analysis level the computed raw
    performance measures are analyzed using exploratory and
    inferential methods. This package is mainly concerned with the
    Analysis level; in what the derivation of a statistically correct
    order of the candidate algorithms is a major objective.
  </p>


  <h2>Demonstration</h2>

  <p>
    <a href="http://www.statistik.lmu.de/~eugster/publications/thesis-2011-phd.pdf#page=41"><img src="beplot.png" /></a>
    <a href="http://www.statistik.lmu.de/~eugster/publications/thesis-2011-phd.pdf#page=82"><img src="bsgraph.png" /></a>
    <a href="http://www.statistik.lmu.de/~eugster/publications/thesis-2011-phd.pdf#page=89"><img src="atypes.png" /></a>
  </p>
  

  <h3>Explorative and inferential analysis of benchmark experiments</h3>

  <p>
    See for example the demos <tt>benchplot</tt>
    and <tt>lsbenchplot-cs621</tt> for the analysis of six common
    learning algorithms
    on <a href="http://archive.ics.uci.edu/ml/">UCI</a> data sets:
    <blockquote>
    <pre>
R> demo("benchplot")
R> demo("lsbenchplot-cs621")
    </pre>
    </blockquote>
  </p>


  <h3>Archetypal analysis of benchmark experiments</h3>

  <p>
    In general, comparing algorithms often means comparing with a
    "best" or "worst" algorithm, i.e., comparing with an extreme
    algorithm (the benchmark). However, in case of of more than one
    performance measure or more than one data set no uniquely defined
    extreme values are
    available---<a href="http://archetypes.r-forge.r-project.org/">archetypal
    analysis</a> can be used to compute data-driven benchmark algorithms:
    <blockquote>
    <pre>
R> demo("lsbenchplot-cs621-atypes")
    </pre>
    </blockquote>
  </p>
  

  <h2>Download</h2>

  <p>
    The <b>stable version</b> of benchmark is available
    on <a href="http://cran.r-project.org/package=benchmark">CRAN</a>;
    issue the following from within R to install it:
    <blockquote>
    <pre>
R> install.packages("benchmark")
    </pre>
    </blockquote>
  </p>

  <p>
    The <b>development version</b> is available on
    <a href="https://r-forge.r-project.org/projects/benchmark/">R-Forge</a>.
  </p>

  <p class="footer">
    Created by <a href="http://www.statistik.lmu.de/~eugster"
    style="color: #C0C0C0">Manuel J. A. Eugster</a>, 2011.
  </p>

</body>
</html>
